{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./env/lib/python3.11/site-packages (1.53.0)\n",
      "Requirement already satisfied: pinecone-client in ./env/lib/python3.11/site-packages (5.0.1)\n",
      "Requirement already satisfied: tiktoken in ./env/lib/python3.11/site-packages (0.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./env/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./env/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./env/lib/python3.11/site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./env/lib/python3.11/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./env/lib/python3.11/site-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./env/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./env/lib/python3.11/site-packages (from pinecone-client) (2024.8.30)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in ./env/lib/python3.11/site-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in ./env/lib/python3.11/site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in ./env/lib/python3.11/site-packages (from pinecone-client) (2.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./env/lib/python3.11/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./env/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./env/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install openai pinecone-client tiktoken\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import tiktoken\n",
    "import functools\n",
    "\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "TIMEOUT = 10      # Timeout for API calls\n",
    "MAX_RETRIES = 3   # For API calls\n",
    "BATCH_SIZE = 100  # For batch processing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/home/prakhar/Documents/personal/jupiter_chatbot/env/lib/python3.11/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n"
     ]
    }
   ],
   "source": [
    "# API Keys and Configuration\n",
    "OPENAI_API_KEY = 'your-openai-api-key'\n",
    "PINECONE_API_KEY = 'your-pinecone-api-key'\n",
    "PINECONE_CLOUD = 'aws'\n",
    "PINECONE_REGION = 'us-east-1'\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Constants\n",
    "MAX_TOKENS = 225\n",
    "PINECONE_INDEX_NAME = 'challenge-index'\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL = \"gpt-4o\"\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "try:\n",
    "    index = pc.Index(PINECONE_INDEX_NAME)\n",
    "except:\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=PINECONE_CLOUD,\n",
    "            region=PINECONE_REGION\n",
    "        )\n",
    "    )\n",
    "    index = pc.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversation history\n",
    "history = [\n",
    "    \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
    "    \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
    "    \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
    "    \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
    "    \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
    "    \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
    "    \"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
    "    \"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
    "    \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
    "    \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
    "    \"11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.\",\n",
    "    \"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\",\n",
    "    \"13: User: Nice! Any good board games? | Bot: Settlers of Catan and Ticket to Ride are both excellent choices.\",\n",
    "    \"14: User: I'll try them out. Any movie recommendations? | Bot: 'Inception' and 'The Matrix' are must-watch movies.\",\n",
    "    \"15: User: I love those movies! Any TV shows? | Bot: 'Breaking Bad' and 'Stranger Things' are very popular.\",\n",
    "    \"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\",\n",
    "    \"17: User: Thanks! What are some good travel destinations? | Bot: Paris, Tokyo, and Bali are amazing travel spots.\",\n",
    "    \"18: User: I'll add them to my list. Any packing tips? | Bot: Roll your clothes to save space and use packing cubes.\",\n",
    "    \"19: User: That's helpful! What about travel insurance? | Bot: Always get travel insurance for safety and peace of mind.\",\n",
    "    \"20: User: Thanks for the tips! Any last advice? | Bot: Just enjoy your journey and make the most out of your experiences.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings_to_pinecone(history: List[str], index_name: str = 'challenge-index') -> None:\n",
    "    \"\"\"add conversation history embeddings to vector db\"\"\"\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    @functools.lru_cache(maxsize=1000)\n",
    "    def get_embedding(text: str) -> List[float]:\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                embedding = client.embeddings.create(\n",
    "                    model=EMBEDDING_MODEL,\n",
    "                    input=text,\n",
    "                    timeout=TIMEOUT\n",
    "                ).data[0].embedding\n",
    "                return embedding\n",
    "            except Exception as e:\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    logger.error(f\"Failed to get embedding after {MAX_RETRIES} attempts: {e}\")\n",
    "                    raise\n",
    "                time.sleep(2 ** attempt)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(history), BATCH_SIZE):\n",
    "        batch = history[i:i + BATCH_SIZE]\n",
    "        vectors_to_upsert = []\n",
    "        \n",
    "        # Parallel embedding generation\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            embeddings = list(executor.map(get_embedding, batch))\n",
    "        \n",
    "        for message, embedding in zip(batch, embeddings):\n",
    "            msg_num = message.split(':')[0]\n",
    "            vector = {\n",
    "                'id': f'msg_{msg_num}',\n",
    "                'values': embedding,\n",
    "                'metadata': {\n",
    "                    'text': message,\n",
    "                    'msg_num': int(msg_num),\n",
    "                    'timestamp': datetime.utcnow().isoformat()\n",
    "                }\n",
    "            }\n",
    "            vectors_to_upsert.append(vector)\n",
    "        \n",
    "        # Upsert\n",
    "        try:\n",
    "            index = pc.Index(index_name)\n",
    "            index.upsert(vectors=vectors_to_upsert)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to upsert vectors: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Verify the index stats after upload\n",
    "    try:\n",
    "        index = pc.Index(index_name)\n",
    "        stats = index.describe_index_stats()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get index stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=100)\n",
    "\n",
    "def retrieve_relevant_history(query: str, index_name: str = 'challenge-index', top_k: int = 5) -> List[str]:\n",
    "    \"\"\"retrieve relevant messages using hybrid search and reranking\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    try:\n",
    "        index = pc.Index(index_name)\n",
    "        \n",
    "        # Get query embedding with retry logic\n",
    "        query_embedding = client.embeddings.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=query,\n",
    "            timeout=TIMEOUT\n",
    "        ).data[0].embedding\n",
    "        \n",
    "        # Hybrid search combining semantic and keyword matching\n",
    "        results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k * 2,\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "        # Rerank results using relevance scoring\n",
    "        scored_results = []\n",
    "        for match in results['matches']:\n",
    "            # Calculate hybrid score\n",
    "            semantic_score = match.score\n",
    "            text_similarity = _calculate_text_similarity(query, match.metadata['text'])\n",
    "            temporal_score = _calculate_temporal_score(match.metadata['timestamp'])\n",
    "            \n",
    "            final_score = (semantic_score * 0.6 + \n",
    "                         text_similarity * 0.3 + \n",
    "                         temporal_score * 0.1)\n",
    "            \n",
    "            scored_results.append((match, final_score))\n",
    "        \n",
    "        # Sort by final score and take top_k\n",
    "        scored_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        relevant_messages = [match[0].metadata['text'] for match in scored_results[:top_k]]\n",
    "        \n",
    "        return relevant_messages\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving history: {e}\")\n",
    "        return []\n",
    "\n",
    "def _calculate_text_similarity(query: str, text: str) -> float:\n",
    "    \"\"\"Calculate simple text similarity score\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    text_words = set(text.lower().split())\n",
    "    return len(query_words.intersection(text_words)) / len(query_words)\n",
    "\n",
    "def _calculate_temporal_score(timestamp: str) -> float:\n",
    "    \"\"\"Calculate recency score\"\"\"\n",
    "    time_diff = datetime.utcnow() - datetime.fromisoformat(timestamp)\n",
    "    hours_old = time_diff.total_seconds() / 3600\n",
    "    return max(0, 1 - (hours_old / 24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(query: str, history: List[str], index_name: str = 'challenge-index'):\n",
    "    \"\"\"prepare the prompt for chatbot\"\"\"\n",
    "    \n",
    "    relevant_messages = retrieve_relevant_history(query, index_name)\n",
    "    \n",
    "    # init tokenizer\n",
    "    encoding = tiktoken.encoding_for_model(CHAT_MODEL)\n",
    "    \n",
    "    # Extract and sort conversation pairs\n",
    "    conversation_pairs = []\n",
    "    for message in relevant_messages:\n",
    "        parts = message.split(' | ')\n",
    "        if len(parts) == 2:\n",
    "            msg_num = int(parts[0].split(':')[0])\n",
    "            conversation_pairs.append((msg_num, parts[0].split(': ', 1)[1], parts[1].split(': ', 1)[1]))\n",
    "    \n",
    "    conversation_pairs.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Format context while respecting token limit\n",
    "    context_parts = []\n",
    "    total_tokens = 0\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Use the context provided to give relevant and consistent responses. \"\n",
    "        \"Keep responses concise.\"\n",
    "    )\n",
    "    \n",
    "    # Count system prompt tokens\n",
    "    total_tokens += len(encoding.encode(system_prompt))\n",
    "    total_tokens += len(encoding.encode(query))\n",
    "    \n",
    "\n",
    "    for _, user, bot in conversation_pairs:\n",
    "        pair_text = f\"User: {user}\\nAssistant: {bot}\\n\"\n",
    "        pair_tokens = len(encoding.encode(pair_text))\n",
    "        \n",
    "        if total_tokens + pair_tokens > MAX_TOKENS - 50:\n",
    "            break\n",
    "            \n",
    "        context_parts.append(pair_text)\n",
    "        total_tokens += pair_tokens\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{system_prompt}\\n\\nPrevious relevant conversation:\\n{context}\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    return messages, relevant_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Prompt: what was mountain name we discussed before?\n",
      "Context Used: ['11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.', '21: User: what lunch you have suggested me before? | Bot: I suggested trying a quinoa salad with avocado and chicken for lunch.', \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\", \"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\", \"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\"]\n",
      "Response: We discussed the Rockies and the Appalachian Mountains.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_final_prompt():\n",
    "    \"\"\"test the final prompt\"\"\"\n",
    "    \n",
    "    final_test_prompt = \"what was mountain name we discussed before?\"\n",
    "    \n",
    "    # prepare prompt\n",
    "    messages, context_referred = prepare_prompt(final_test_prompt, history)\n",
    "\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    bot_response = response.choices[0].message.content\n",
    "\n",
    "    new_msg_num = len(history) + 1\n",
    "    new_conversation = f\"{new_msg_num}: User: {final_test_prompt} | Bot: {bot_response}\"\n",
    "    \n",
    "    # add new conversation to vector db\n",
    "    add_embeddings_to_pinecone([new_conversation])\n",
    "    \n",
    "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
    "    print(f\"Context Used: {context_referred}\")\n",
    "    print(f\"Response: {bot_response}\")\n",
    "\n",
    "test_final_prompt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
